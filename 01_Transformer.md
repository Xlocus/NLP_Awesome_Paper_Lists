# Transformer
1. [cosFormer: Rethinking Softmax in Attention](https://arxiv.org/abs/2202.08791) (ICLR 2022)
2. [Transformer Quality in Linear Time](https://arxiv.org/abs/2202.10447) (2022)
