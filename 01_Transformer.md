# Transformer
1. [cosFormer: Rethinking Softmax in Attention](https://arxiv.org/abs/2202.08791) (ICLR 2022)
2. [Transformer Quality in Linear Time](https://arxiv.org/abs/2202.10447) (2022)
3. [DeepNet: Scaling Transformers to 1,000 Layers](https://arxiv.org/abs/2203.00555) (2022)
